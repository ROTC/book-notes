\chapter{机器学习基础}
    \section{随机梯度下降}
        梯度下降的计算代价是$O(m)$，随机梯度下降的核心是，梯度是期望。期望可使用小规模的样本近似估计。在算法的每一步，从训练集中均匀抽出一minibatch样本$\mathbb{B} = \{ {\bm{x}^{(1)}},...,{\bm{x}^{(m')}}\}$。
        
        当训练集大小$m$趋向于无限大时，该模型最终会在随机梯度下降抽样到训练集上每个样本前收敛到可能的最优测试误差。继续增加$m$不会延长达到模型可能的最优测试误差的时间。从这点来看，可以认为SGD训练模型的渐近代价是关于$m$的函数的$O(1)$级别。